{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os"
      ],
      "metadata": {
        "id": "1MCAIokGdXer"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jb7VbuoukqQx",
        "outputId": "98a71477-bc1e-471a-c64c-70930d4046c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer\n",
        "import torch\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "dMlI8bMdmVX7"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import ViTFeatureExtractor, VisionEncoderDecoderModel, AutoTokenizer\n",
        "\n",
        "def process_image(image_path):\n",
        "    model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "    feature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    \n",
        "    max_length = 16\n",
        "    num_beams = 4\n",
        "    gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
        "\n",
        "    def predict_step(image_paths, num_captions=5):\n",
        "        images = []\n",
        "        for image_path in image_paths:\n",
        "            i_image = Image.open(image_path)\n",
        "            if i_image.mode != \"RGB\":\n",
        "                i_image = i_image.convert(mode=\"RGB\")\n",
        "\n",
        "            images.append(i_image)\n",
        "\n",
        "        pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
        "        pixel_values = pixel_values.to(device)\n",
        "\n",
        "        preds = []\n",
        "        with torch.no_grad():\n",
        "            for _ in range(num_captions):\n",
        "                output_ids = model.generate(pixel_values, do_sample=True, top_k=50, **gen_kwargs)\n",
        "                captions = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "                captions = list(set([caption.strip() for caption in captions]))  # Remove duplicate captions\n",
        "                preds.append(captions)\n",
        "\n",
        "        return preds\n",
        "\n",
        "    predictions = predict_step([image_path], num_captions=3)\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "wZWlwmZyYnWX"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = process_image('Image2.png')\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLX02G73YqS6",
        "outputId": "5842bc7c-cbeb-461b-e9fd-db84a9be19c1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at nlpconnect/vit-gpt2-image-captioning were not used when initializing VisionEncoderDecoderModel: ['decoder.transformer.h.1.crossattention.bias', 'decoder.transformer.h.2.attn.masked_bias', 'decoder.transformer.h.11.crossattention.masked_bias', 'decoder.transformer.h.11.attn.bias', 'decoder.transformer.h.10.crossattention.masked_bias', 'decoder.transformer.h.9.crossattention.bias', 'decoder.transformer.h.1.crossattention.masked_bias', 'decoder.transformer.h.4.attn.bias', 'decoder.transformer.h.7.attn.bias', 'decoder.transformer.h.0.attn.masked_bias', 'decoder.transformer.h.5.crossattention.bias', 'decoder.transformer.h.10.attn.masked_bias', 'decoder.transformer.h.5.crossattention.masked_bias', 'decoder.transformer.h.8.crossattention.bias', 'decoder.transformer.h.11.attn.masked_bias', 'decoder.transformer.h.2.crossattention.masked_bias', 'decoder.transformer.h.3.crossattention.bias', 'decoder.transformer.h.7.attn.masked_bias', 'decoder.transformer.h.10.attn.bias', 'decoder.transformer.h.1.attn.masked_bias', 'decoder.transformer.h.9.crossattention.masked_bias', 'decoder.transformer.h.8.attn.bias', 'decoder.transformer.h.4.crossattention.masked_bias', 'decoder.transformer.h.6.attn.masked_bias', 'decoder.transformer.h.4.attn.masked_bias', 'decoder.transformer.h.10.crossattention.bias', 'decoder.transformer.h.6.attn.bias', 'decoder.transformer.h.0.attn.bias', 'decoder.transformer.h.8.attn.masked_bias', 'decoder.transformer.h.2.crossattention.bias', 'decoder.transformer.h.6.crossattention.bias', 'decoder.transformer.h.4.crossattention.bias', 'decoder.transformer.h.6.crossattention.masked_bias', 'decoder.transformer.h.9.attn.masked_bias', 'decoder.transformer.h.3.attn.bias', 'decoder.transformer.h.1.attn.bias', 'decoder.transformer.h.3.crossattention.masked_bias', 'decoder.transformer.h.7.crossattention.bias', 'decoder.transformer.h.0.crossattention.masked_bias', 'decoder.transformer.h.5.attn.masked_bias', 'decoder.transformer.h.0.crossattention.bias', 'decoder.transformer.h.11.crossattention.bias', 'decoder.transformer.h.3.attn.masked_bias', 'decoder.transformer.h.9.attn.bias', 'decoder.transformer.h.7.crossattention.masked_bias', 'decoder.transformer.h.8.crossattention.masked_bias', 'decoder.transformer.h.5.attn.bias', 'decoder.transformer.h.2.attn.bias']\n",
            "- This IS expected if you are initializing VisionEncoderDecoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing VisionEncoderDecoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['a woman standing in a field with a horse'], ['a woman standing in a field with a horse'], ['a woman is standing in a field with a horse']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "API"
      ],
      "metadata": {
        "id": "j81y-DeTYyoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fastapi\n",
        "!pip install uvicorn\n",
        "!pip install pickle5\n",
        "!pip install pydantic\n",
        "!pip install scikit-learn\n",
        "!pip install requests\n",
        "!pip install pypi-json\n",
        "!pip install pyngrok\n",
        "!pip install nest-asyncio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhTuDsY1YslA",
        "outputId": "044b0a33-6efb-44d9-d48e-c2dc34e14155"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (0.96.0)\n",
            "Requirement already satisfied: pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2 in /usr/local/lib/python3.10/dist-packages (from fastapi) (1.10.7)\n",
            "Requirement already satisfied: starlette<0.28.0,>=0.27.0 in /usr/local/lib/python3.10/dist-packages (from fastapi) (0.27.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.7,!=1.7.1,!=1.7.2,!=1.7.3,!=1.8,!=1.8.1,<2.0.0,>=1.6.2->fastapi) (4.5.0)\n",
            "Requirement already satisfied: anyio<5,>=3.4.0 in /usr/local/lib/python3.10/dist-packages (from starlette<0.28.0,>=0.27.0->fastapi) (3.6.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.4.0->starlette<0.28.0,>=0.27.0->fastapi) (1.3.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: uvicorn in /usr/local/lib/python3.10/dist-packages (0.22.0)\n",
            "Requirement already satisfied: click>=7.0 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (8.1.3)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn) (0.14.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pickle5 in /usr/local/lib/python3.10/dist-packages (0.0.11)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.10/dist-packages (1.10.7)\n",
            "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from pydantic) (4.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.10.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.1.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pypi-json in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: apeye>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from pypi-json) (1.4.0)\n",
            "Requirement already satisfied: packaging>=21.0 in /usr/local/lib/python3.10/dist-packages (from pypi-json) (23.1)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from pypi-json) (2.27.1)\n",
            "Requirement already satisfied: apeye-core>=1.0.0b2 in /usr/local/lib/python3.10/dist-packages (from apeye>=1.1.0->pypi-json) (1.1.4)\n",
            "Requirement already satisfied: domdf-python-tools>=2.6.0 in /usr/local/lib/python3.10/dist-packages (from apeye>=1.1.0->pypi-json) (3.6.1)\n",
            "Requirement already satisfied: platformdirs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from apeye>=1.1.0->pypi-json) (3.3.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->pypi-json) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->pypi-json) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->pypi-json) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->pypi-json) (3.4)\n",
            "Requirement already satisfied: natsort>=7.0.1 in /usr/local/lib/python3.10/dist-packages (from domdf-python-tools>=2.6.0->apeye>=1.1.0->pypi-json) (8.3.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.1 in /usr/local/lib/python3.10/dist-packages (from domdf-python-tools>=2.6.0->apeye>=1.1.0->pypi-json) (4.5.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.10/dist-packages (6.0.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (1.5.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-multipart"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfQZKc3XY2Jr",
        "outputId": "e32810d4-0a26-4e2c-8495-03dd416e8a05"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (0.0.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from fastapi import FastAPI, File, UploadFile, HTTPException\n",
        "from pydantic import BaseModel\n",
        "import io\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "from fastapi.middleware.cors import CORSMiddleware\n",
        "from starlette.responses import JSONResponse\n",
        "from pyngrok import ngrok\n",
        "from secrets import token_hex\n",
        "from fastapi.responses import FileResponse\n",
        "import nest_asyncio\n",
        "import uvicorn\n",
        "app = FastAPI()\n",
        "origins = [\"*\"]\n",
        "app.add_middleware(\n",
        "    CORSMiddleware,\n",
        "    allow_origins=origins,\n",
        "    allow_credentials=True,\n",
        "    allow_methods=[\"*\"],\n",
        "    allow_headers=[\"*\"],\n",
        ")\n",
        "\n",
        "# def process_image(image_path):\n",
        "#     model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "#     feature_extractor = ViTFeatureExtractor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "#     tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
        "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "#     model.to(device)\n",
        "    \n",
        "#     max_length = 16\n",
        "#     num_beams = 4\n",
        "#     gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
        "\n",
        "#     def predict_step(image_paths, num_captions=3):\n",
        "#         images = []\n",
        "#         for image_path in image_paths:\n",
        "#             i_image = Image.open(image_path)\n",
        "#             if i_image.mode != \"RGB\":\n",
        "#                 i_image = i_image.convert(mode=\"RGB\")\n",
        "\n",
        "#             images.append(i_image)\n",
        "\n",
        "#         pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
        "#         pixel_values = pixel_values.to(device)\n",
        "\n",
        "#         preds = []\n",
        "#         with torch.no_grad():\n",
        "#             for _ in range(num_captions):\n",
        "#                 output_ids = model.generate(pixel_values, do_sample=True, top_k=50, **gen_kwargs)\n",
        "#                 captions = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "#                 captions = list(set([caption.strip() for caption in captions]))  # Remove duplicate captions\n",
        "#                 preds.append(captions)\n",
        "\n",
        "#         return preds\n",
        "\n",
        "#     predictions = predict_step([image_path], num_captions=5)\n",
        "#     return predictions\n",
        "\n",
        "@app.post(\"/image\")\n",
        "async def image(image: UploadFile = File(...)):\n",
        "    contents = await image.read()\n",
        "    image_path = \"temp_image.jpg\"  # Temporary path to save the uploaded image\n",
        "    with open(image_path, \"wb\") as f:\n",
        "        f.write(contents)\n",
        "\n",
        "    # Process the image and get the predictions\n",
        "    result = process_image(image_path)\n",
        "\n",
        "    # Clean up the temporary image file\n",
        "    os.remove(image_path)\n",
        "\n",
        "    return JSONResponse({\"predictions\": result})"
      ],
      "metadata": {
        "id": "Cke23kx5Y-Jp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nest_asyncio.apply()\n",
        "public_url = ngrok.connect(8000).public_url\n",
        "print(f'Public URL: {public_url}')\n",
        "uvicorn.run(app, host='0.0.0.0', port=8000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxt0u3cbdcNF",
        "outputId": "84efb58f-cbe8-4a58-e82d-294558abd278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2023-06-09T09:21:34+0000 lvl=warn msg=\"ngrok config file found at legacy location, move to XDG location\" xdg_path=/root/.config/ngrok/ngrok.yml legacy_path=/root/.ngrok2/ngrok.yml\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: https://34f5-35-247-121-94.ngrok.io\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [246]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n",
            "Some weights of the model checkpoint at nlpconnect/vit-gpt2-image-captioning were not used when initializing VisionEncoderDecoderModel: ['decoder.transformer.h.1.crossattention.bias', 'decoder.transformer.h.2.attn.masked_bias', 'decoder.transformer.h.11.crossattention.masked_bias', 'decoder.transformer.h.11.attn.bias', 'decoder.transformer.h.10.crossattention.masked_bias', 'decoder.transformer.h.9.crossattention.bias', 'decoder.transformer.h.1.crossattention.masked_bias', 'decoder.transformer.h.4.attn.bias', 'decoder.transformer.h.7.attn.bias', 'decoder.transformer.h.0.attn.masked_bias', 'decoder.transformer.h.5.crossattention.bias', 'decoder.transformer.h.10.attn.masked_bias', 'decoder.transformer.h.5.crossattention.masked_bias', 'decoder.transformer.h.8.crossattention.bias', 'decoder.transformer.h.11.attn.masked_bias', 'decoder.transformer.h.2.crossattention.masked_bias', 'decoder.transformer.h.3.crossattention.bias', 'decoder.transformer.h.7.attn.masked_bias', 'decoder.transformer.h.10.attn.bias', 'decoder.transformer.h.1.attn.masked_bias', 'decoder.transformer.h.9.crossattention.masked_bias', 'decoder.transformer.h.8.attn.bias', 'decoder.transformer.h.4.crossattention.masked_bias', 'decoder.transformer.h.6.attn.masked_bias', 'decoder.transformer.h.4.attn.masked_bias', 'decoder.transformer.h.10.crossattention.bias', 'decoder.transformer.h.6.attn.bias', 'decoder.transformer.h.0.attn.bias', 'decoder.transformer.h.8.attn.masked_bias', 'decoder.transformer.h.2.crossattention.bias', 'decoder.transformer.h.6.crossattention.bias', 'decoder.transformer.h.4.crossattention.bias', 'decoder.transformer.h.6.crossattention.masked_bias', 'decoder.transformer.h.9.attn.masked_bias', 'decoder.transformer.h.3.attn.bias', 'decoder.transformer.h.1.attn.bias', 'decoder.transformer.h.3.crossattention.masked_bias', 'decoder.transformer.h.7.crossattention.bias', 'decoder.transformer.h.0.crossattention.masked_bias', 'decoder.transformer.h.5.attn.masked_bias', 'decoder.transformer.h.0.crossattention.bias', 'decoder.transformer.h.11.crossattention.bias', 'decoder.transformer.h.3.attn.masked_bias', 'decoder.transformer.h.9.attn.bias', 'decoder.transformer.h.7.crossattention.masked_bias', 'decoder.transformer.h.8.crossattention.masked_bias', 'decoder.transformer.h.5.attn.bias', 'decoder.transformer.h.2.attn.bias']\n",
            "- This IS expected if you are initializing VisionEncoderDecoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing VisionEncoderDecoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     27.97.226.53:0 - \"POST /image HTTP/1.1\" 200 OK\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at nlpconnect/vit-gpt2-image-captioning were not used when initializing VisionEncoderDecoderModel: ['decoder.transformer.h.1.crossattention.bias', 'decoder.transformer.h.2.attn.masked_bias', 'decoder.transformer.h.11.crossattention.masked_bias', 'decoder.transformer.h.11.attn.bias', 'decoder.transformer.h.10.crossattention.masked_bias', 'decoder.transformer.h.9.crossattention.bias', 'decoder.transformer.h.1.crossattention.masked_bias', 'decoder.transformer.h.4.attn.bias', 'decoder.transformer.h.7.attn.bias', 'decoder.transformer.h.0.attn.masked_bias', 'decoder.transformer.h.5.crossattention.bias', 'decoder.transformer.h.10.attn.masked_bias', 'decoder.transformer.h.5.crossattention.masked_bias', 'decoder.transformer.h.8.crossattention.bias', 'decoder.transformer.h.11.attn.masked_bias', 'decoder.transformer.h.2.crossattention.masked_bias', 'decoder.transformer.h.3.crossattention.bias', 'decoder.transformer.h.7.attn.masked_bias', 'decoder.transformer.h.10.attn.bias', 'decoder.transformer.h.1.attn.masked_bias', 'decoder.transformer.h.9.crossattention.masked_bias', 'decoder.transformer.h.8.attn.bias', 'decoder.transformer.h.4.crossattention.masked_bias', 'decoder.transformer.h.6.attn.masked_bias', 'decoder.transformer.h.4.attn.masked_bias', 'decoder.transformer.h.10.crossattention.bias', 'decoder.transformer.h.6.attn.bias', 'decoder.transformer.h.0.attn.bias', 'decoder.transformer.h.8.attn.masked_bias', 'decoder.transformer.h.2.crossattention.bias', 'decoder.transformer.h.6.crossattention.bias', 'decoder.transformer.h.4.crossattention.bias', 'decoder.transformer.h.6.crossattention.masked_bias', 'decoder.transformer.h.9.attn.masked_bias', 'decoder.transformer.h.3.attn.bias', 'decoder.transformer.h.1.attn.bias', 'decoder.transformer.h.3.crossattention.masked_bias', 'decoder.transformer.h.7.crossattention.bias', 'decoder.transformer.h.0.crossattention.masked_bias', 'decoder.transformer.h.5.attn.masked_bias', 'decoder.transformer.h.0.crossattention.bias', 'decoder.transformer.h.11.crossattention.bias', 'decoder.transformer.h.3.attn.masked_bias', 'decoder.transformer.h.9.attn.bias', 'decoder.transformer.h.7.crossattention.masked_bias', 'decoder.transformer.h.8.crossattention.masked_bias', 'decoder.transformer.h.5.attn.bias', 'decoder.transformer.h.2.attn.bias']\n",
            "- This IS expected if you are initializing VisionEncoderDecoderModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing VisionEncoderDecoderModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:     27.97.226.53:0 - \"POST /image HTTP/1.1\" 200 OK\n"
          ]
        }
      ]
    }
  ]
}